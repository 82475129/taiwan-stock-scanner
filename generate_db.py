import requests
import re
from bs4 import BeautifulSoup
import json
import time

DB_FILE = "taiwan_electronic_stocks.json"

# å®šç¾©é›»å­è‚¡åˆ†é¡ ID (Yahoo è‚¡å¸‚)
SECTOR_MAP = {
    "TAI": {40: "åŠå°é«”", 41: "é›»è…¦é€±é‚Š", 42: "å…‰é›»", 43: "é€šä¿¡ç¶²è·¯", 44: "é›»å­é›¶çµ„ä»¶", 45: "é›»å­é€šè·¯", 46: "è³‡è¨Šæœå‹™", 47: "å…¶ä»–é›»å­"},
    "TWO": {153: "åŠå°é«”", 154: "é›»è…¦é€±é‚Š", 155: "å…‰é›»", 156: "é€šä¿¡ç¶²è·¯", 157: "é›»å­é›¶çµ„ä»¶", 158: "é›»å­é€šè·¯", 159: "è³‡è¨Šæœå‹™", 160: "å…¶ä»–é›»å­"}
}

def start_crawling():
    full_db = {}
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0'}
    
    print("ğŸš€ é–‹å§‹æŠ“å–å…¨å°è‚¡é›»å­é¡è‚¡æ¸…å–®...")
    
    for exchange, sectors in SECTOR_MAP.items():
        for sid, name in sectors.items():
            url = f"https://tw.stock.yahoo.com/class-quote?sectorId={sid}&exchange={exchange}"
            try:
                resp = requests.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # æŠ“å–è‚¡ç¥¨ä»£è™Ÿèˆ‡åç¨±
                # çµæ§‹é€šå¸¸æ˜¯ï¼šä»£è™Ÿåœ¨ <span>, åç¨±åœ¨ <div>
                rows = soup.select('div[class*="table-row"]')
                for row in rows:
                    code_tag = row.select_one('span[class*="C(#7c7e80)"]')
                    name_tag = row.select_one('div[class*="Lh(20px)"]')
                    
                    if code_tag and name_tag:
                        ticker = code_tag.get_text(strip=True)
                        stock_name = name_tag.get_text(strip=True)
                        
                        # æ ¼å¼åŒ–ç‚º yfinance æ ¼å¼
                        suffix = ".TW" if exchange == "TAI" else ".TWO"
                        full_db[f"{ticker}{suffix}"] = stock_name
                
                print(f"âœ… å·²å®Œæˆ: {exchange} {name}")
                time.sleep(1) # ç¦®è²Œæ€§å»¶é²
            except Exception as e:
                print(f"âŒ æŠ“å–å¤±æ•— {name}: {e}")

    # å„²å­˜åˆ° JSON
    with open(DB_FILE, 'w', encoding='utf-8') as f:
        json.dump(full_db, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ¨ æŠ“å–å®Œæˆï¼ç¸½è¨ˆ {len(full_db)} æª”é›»å­è‚¡å·²å­˜å…¥ {DB_FILE}")

if __name__ == "__main__":
    start_crawling()
